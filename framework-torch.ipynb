{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from time import localtime, strftime\n",
    "\n",
    "import lutorpy as lua\n",
    "require(\"nn\")\n",
    "require(\"optim\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "\n",
    "# Number of samples in each class\n",
    "\n",
    "TEST_SAMPLE_NUMBER = 51731\n",
    "TRAIN_SAMPLE_NUMBER = 257187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ZERO_PADDING = 6\n",
    "\n",
    "def get_filename(index):\n",
    "    return \"src/Csv/{}.txt\".format(str(index).zfill(ZERO_PADDING))\n",
    "\n",
    "def get_framename(index):\n",
    "    return \"src/Frame/{}.jpg\".format(str(index).zfill(ZERO_PADDING))\n",
    "\n",
    "NUMBER_OF_PEDESTRIANS = 12273\n",
    "\n",
    "def download_pedestrian(index):\n",
    "    assert(0 <= index < NUMBER_OF_PEDESTRIANS), \"pedestrian number should be between 0 and {max}; given number: {id}\".format(\n",
    "        max=NUMBER_OF_PEDESTRIANS-1, id=index)\n",
    "    filename = get_filename(index)\n",
    "    data = pd.read_csv(filename, index_col=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_view(data, length, only_odd_rows=True):\n",
    "    data = np.array(data).flatten()\n",
    "    len1 = len(data)\n",
    "    len2 = length\n",
    "    return np.lib.stride_tricks.as_strided(data, shape=(len1 - len2 + 1, len2),\n",
    "                                                    strides=(data.dtype.itemsize,) * 2)[::2 if only_odd_rows else 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# array1: [sX1; sY1; sX2; sY2; ...]\n",
    "# array2: [eX1; eY1; eX2; eY2; ...]\n",
    "# output: [dist((sX1, sY1), (eX1, eY1)), dist((sX1, sY1), (eX1, eY1)),\n",
    "#          dist((sX2, sY2), (eX2, eY2)), dist((sX2, sY2), (eX2, eY2)), ...]\n",
    "\n",
    "def distance_for_each_point(array1, array2):\n",
    "    array_length = len(array1)\n",
    "    len2 = len(array2)\n",
    "    assert (array_length == len2), \"Arrays' sizes have to be equal (array1: {}, array2: {})\".format(array_length, len2)\n",
    "    \n",
    "    if array1.ndim == 1:\n",
    "        distance = np.linalg.norm((array1 - array2).reshape((int(array_length / 2), 2)), axis=1)\n",
    "        result = np.array([[d, d] for d in distance]).flatten()\n",
    "    else:\n",
    "        result = np.array([distance_for_each_point(array1[i], array2[i]) for i in range(array_length)])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# metrics between real results (in tests) and predicted\n",
    "def distance(test_results, predicted_results):\n",
    "    return distance_for_each_point(np.array(test_results), predicted_results).sum() / TEST_SAMPLE_NUMBER / 2\n",
    "\n",
    "# score between real results (in tests) and predicted\n",
    "def score(test_results, predicted_results, baseline_results):\n",
    "    test_results = np.array(test_results)\n",
    "    predicted_results = np.array(predicted_results)\n",
    "    baseline_results = np.array(baseline_results)\n",
    "    return (1 - ((test_results - predicted_results) ** 2).sum()/((test_results - baseline_results) ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data may be present as [n_features * n_samples] or [n_samples * n_features] \n",
    "# usually algorithms require second variant but I prefer first\n",
    "\n",
    "def to_model(df):\n",
    "    df = np.array(df).T\n",
    "    return df.reshape((int(df.shape[0] / 2), int(df.shape[1] * 2)))\n",
    "\n",
    "def to_model_with_features(df):\n",
    "    df = np.array(df)\n",
    "    movement_vector = np.diff(df,axis=0)\n",
    "    speed = np.power(movement_vector, 2)\n",
    "    speed[:,::2] = -speed[:,::2]\n",
    "    speed = np.sqrt(np.diff(speed)[:,::2])\n",
    "    double_speed = np.zeros_like(movement_vector, dtype=np.float64)\n",
    "    double_speed[:,::2] = double_speed[:,1::2] = speed\n",
    "    last_speed = speed[-1:]\n",
    "    mean_speed = np.mean(speed, axis=0)\n",
    "    cort_coord = movement_vector / double_speed\n",
    "    angle = np.arctan2(cort_coord[:,::2], cort_coord[:,1::2]) / np.pi * 4\n",
    "    \n",
    "    return (np.concatenate((to_model(df), to_model(movement_vector), speed.T, last_speed.T, np.array([mean_speed]).T,\n",
    "                            angle.T), axis=1)).astype(int)\n",
    "def from_model(npa):\n",
    "    return npa.reshape((int(npa.shape[0] * 2), int(npa.shape[1] / 2))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first simply algorithm to get some start\n",
    "# more about it you can find in 'baseline_distance_between_real_points_and_predicted.ipynb'\n",
    "def baseline(test_data, start_point_index=0, number_of_points_to_return=5):\n",
    "    error_template = \"Start point index should be less than last point. Start point index: {st}, last point index: {end}\"\n",
    "    assert (start_point_index < len(test_data) - 1), error_template.format(st=start_point_index, end=len(test_data) - 1)\n",
    "    \n",
    "    start_point = np.array(test_data)[start_point_index]\n",
    "    last_but_one_point = np.array(test_data)[-2]\n",
    "    end_point = np.array(test_data)[-1]\n",
    "    \n",
    "    distance = distance_for_each_point(end_point, start_point)\n",
    "    normalized_motion_vector = (end_point - start_point) / distance\n",
    "    normalized_motion_vector[np.where(distance == 0)] = 0\n",
    "    last_vector_length = distance_for_each_point(end_point, last_but_one_point)\n",
    "    \n",
    "    motion_vector = normalized_motion_vector * last_vector_length\n",
    "    result = []\n",
    "    for i in range(number_of_points_to_return):\n",
    "        result.append(end_point + (i + 1) * motion_vector)\n",
    "        \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_results(function, test_results, parameter_name, list_of_values, **other_parameters):\n",
    "    result = [] # we will keep results for each configuration here\n",
    "\n",
    "    for i, value in enumerate(list_of_values):\n",
    "        print(\"{cur}/{num}: {t}\".format(cur=i+1, num=len(list_of_values), t=strftime(\"%Y-%m-%d %H:%M:%S\", localtime())))\n",
    "        other_parameters[parameter_name] = value        \n",
    "        predicted_results = function(**other_parameters)\n",
    "        np.save(\"src/Logs/{date}_predicted_coordinates_{function_name}_{parameter_name}_\".format(\n",
    "                date=strftime(\"%Y%m%d\", localtime()), function_name=function.__name__, parameter_name=parameter_name)\n",
    "                + str(value), predicted_results)\n",
    "        result.append(score(test_results, predicted_results, baseline(other_parameters['test_data'], start_point_index=2)))\n",
    "        \n",
    "    print(\"done! {time}\".format(time=strftime(\"%Y-%m-%d %H:%M:%S\", localtime())))\n",
    "    print(\"Results: {}\".format(result))\n",
    "        \n",
    "    ind = list_of_values\n",
    "    number = len(ind)\n",
    "    width = 2 / number\n",
    "    result_bar = plt.bar(range(number), result, width, color='g')\n",
    "\n",
    "    plt.ylabel('Average difference')\n",
    "    plt.xlabel(parameter_name)\n",
    "    plt.title(\"Difference between real points and predicted by {parameter_name} in {function_name}\".format(\n",
    "        function_name=function.__name__, parameter_name=parameter_name))\n",
    "    plt.xticks(np.array(range(number)) + width/2, ind)\n",
    "    plt.savefig(\"src/Plots/{date}_{function_name}_score_by_{parameter_name}_with_features.png\".format(\n",
    "        date=strftime(\"%Y%m%d\", localtime()), function_name=function.__name__, parameter_name=parameter_name))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading data from .csv files\n",
    "\n",
    "test_data = pd.read_csv('src/test_data_points.csv', index_col=0)\n",
    "train_data = pd.read_csv('src/train_data_points.csv', index_col=0)\n",
    "\n",
    "# split ten frames in input and output data (we want to predict output by input)\n",
    "\n",
    "test_results = test_data[5:10]\n",
    "test_data = test_data[:5]\n",
    "\n",
    "train_results = train_data[5:10]\n",
    "train_data = train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Lua table at 0x401f72e0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model (predictor): \n",
    "model = nn.Sequential()\n",
    "\n",
    "ninputs = to_model_with_features(test_data).shape[1]\n",
    "noutputs = 10\n",
    "\n",
    "model._add(nn.Linear(ninputs, noutputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a loss function to be minimized (mean-square error between predictions and groundtruth labels)\n",
    "criterion = nn.MSECriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x -- parameters (the vector of trainable weights -- all the weights of the linear matrix of our model, plus one bias)\n",
    "# dl_dx -- gradients of our loss function\n",
    "x, dl_dx = model._getParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define index generator for data\n",
    "\n",
    "max_ind = to_model(train_data).shape[0]\n",
    "\n",
    "def generator():\n",
    "    i = 0\n",
    "    while True:\n",
    "        yield i % max_ind\n",
    "        i += 1\n",
    "        \n",
    "ind = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_model = to_model_with_features(train_data)\n",
    "train_results_model = to_model(train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# computes the value of the loss function at a given point x, and the gradient of that function with respect to x\n",
    "def feval(x_new):\n",
    "    # set x to x_new, if differnt\n",
    "    x._copy(x_new)\n",
    "    \n",
    "    # select a new training sample\n",
    "    nidx = next(ind)\n",
    "    \n",
    "    target = torch.fromNumpyArray(train_results_model[nidx].astype(float))\n",
    "    inputs = torch.fromNumpyArray(train_data_model[nidx].astype(float))\n",
    "    \n",
    "    # reset gradients (gradients are always accumulated, to accommodate batch methods)\n",
    "    dl_dx._zero()\n",
    "    \n",
    "    # evaluate the loss function and its derivative wrt x, for that sample\n",
    "    loss_x = criterion._forward(model._forward(inputs), target)\n",
    "    model._backward(inputs, criterion._backward(model.output, target))\n",
    "\n",
    "    # return loss(x) and dloss/dx\n",
    "    return loss_x, dl_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given the function above, we can now easily train the model using SGD.\n",
    "# For that, we need to define four key parameters:\n",
    "# * learning rate: the size of the step taken at each stochasticestimate of the gradient\n",
    "# * weight decay, to regularize the solution (L2 regularization)\n",
    "# * momentum term, to average steps over time\n",
    "# * learning rate decay, to let the algorithm converge more precisely\n",
    "\n",
    "lua.execute(''' sgd_params = {\n",
    "   learningRateDecay = 1e-3,\n",
    "   learningRateDecay = 1e-4,\n",
    "   weightDecay = 0,\n",
    "   momentum = 0\n",
    "} ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LuaError",
     "evalue": "/home/gaydashenko/torch/install/share/lua/5.1/optim/sgd.lua:82: invalid arguments: DoubleTensor number nil \nexpected arguments: *DoubleTensor* [DoubleTensor] double | *DoubleTensor* [DoubleTensor] [double] DoubleTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLuaError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-2b310e05b63e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# * some parameters, which are algorithm-specific\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msgd_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# functions in optim all return two things:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mlutorpy/_lupa.pyx\u001b[0m in \u001b[0;36mlutorpy._lupa._LuaObject.__call__ (lutorpy/_lupa.c:9459)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mlutorpy/_lupa.pyx\u001b[0m in \u001b[0;36mlutorpy._lupa.call_lua (lutorpy/_lupa.c:25877)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mlutorpy/_lupa.pyx\u001b[0m in \u001b[0;36mlutorpy._lupa.execute_lua_call (lutorpy/_lupa.c:25987)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mlutorpy/_lupa.pyx\u001b[0m in \u001b[0;36mlutorpy._lupa.raise_lua_error (lutorpy/_lupa.c:25297)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mLuaError\u001b[0m: /home/gaydashenko/torch/install/share/lua/5.1/optim/sgd.lua:82: invalid arguments: DoubleTensor number nil \nexpected arguments: *DoubleTensor* [DoubleTensor] double | *DoubleTensor* [DoubleTensor] [double] DoubleTensor"
     ]
    }
   ],
   "source": [
    "# We're now good to go... all we have left to do is run over the dataset\n",
    "# for a certain number of iterations, and perform a stochastic update \n",
    "# at each iteration. The number of iterations is found empirically here,\n",
    "# but should typically be determinined using cross-validation.\n",
    "\n",
    "# we cycle over our training data\n",
    "for i in range(100):\n",
    "\n",
    "    # this variable is used to estimate the average loss\n",
    "    current_loss = 0\n",
    "\n",
    "    # an epoch is a full loop over our training data\n",
    "    for i in range(max_ind):\n",
    "\n",
    "        # optim contains several optimization algorithms. \n",
    "        # all of these algorithms assume the same parameters:\n",
    "        # * closure that computes the loss, and its gradient wrt to x, given a point x\n",
    "        # * point x\n",
    "        # * some parameters, which are algorithm-specific\n",
    "      \n",
    "        _,fs = optim.sgd(feval,x,sgd_params)\n",
    "\n",
    "        # functions in optim all return two things:\n",
    "        # * the new x, found by the optimization method (here SGD)\n",
    "        # * the value of the loss functions at all points that were used by the algorithm\n",
    "        # (SGD only estimates the function once, so that list just contains one value)\n",
    "\n",
    "        current_loss = current_loss + fs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
