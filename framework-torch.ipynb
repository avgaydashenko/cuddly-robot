{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from time import localtime, strftime\n",
    "\n",
    "import lutorpy as lua\n",
    "require(\"nn\")\n",
    "require(\"optim\")\n",
    "require(\"cutorch\")\n",
    "require(\"cunn\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "\n",
    "# Number of samples in each class\n",
    "\n",
    "TEST_SAMPLE_NUMBER = 51731\n",
    "TRAIN_SAMPLE_NUMBER = 257187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ZERO_PADDING = 6\n",
    "\n",
    "def get_filename(index):\n",
    "    return \"src/Csv/{}.txt\".format(str(index).zfill(ZERO_PADDING))\n",
    "\n",
    "def get_framename(index):\n",
    "    return \"src/Frame/{}.jpg\".format(str(index).zfill(ZERO_PADDING))\n",
    "\n",
    "NUMBER_OF_PEDESTRIANS = 12273\n",
    "\n",
    "def download_pedestrian(index):\n",
    "    assert(0 <= index < NUMBER_OF_PEDESTRIANS), \"pedestrian number should be between 0 and {max}; given number: {id}\".format(\n",
    "        max=NUMBER_OF_PEDESTRIANS-1, id=index)\n",
    "    filename = get_filename(index)\n",
    "    data = pd.read_csv(filename, index_col=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_view(data, length, only_odd_rows=True):\n",
    "    data = np.array(data).flatten()\n",
    "    len1 = len(data)\n",
    "    len2 = length\n",
    "    return np.lib.stride_tricks.as_strided(data, shape=(len1 - len2 + 1, len2),\n",
    "                                                    strides=(data.dtype.itemsize,) * 2)[::2 if only_odd_rows else 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# array1: [sX1; sY1; sX2; sY2; ...]\n",
    "# array2: [eX1; eY1; eX2; eY2; ...]\n",
    "# output: [dist((sX1, sY1), (eX1, eY1)), dist((sX1, sY1), (eX1, eY1)),\n",
    "#          dist((sX2, sY2), (eX2, eY2)), dist((sX2, sY2), (eX2, eY2)), ...]\n",
    "\n",
    "def distance_for_each_point(array1, array2):\n",
    "    array_length = len(array1)\n",
    "    len2 = len(array2)\n",
    "    assert (array_length == len2), \"Arrays' sizes have to be equal (array1: {}, array2: {})\".format(array_length, len2)\n",
    "    \n",
    "    if array1.ndim == 1:\n",
    "        distance = np.linalg.norm((array1 - array2).reshape((int(array_length / 2), 2)), axis=1)\n",
    "        result = np.array([[d, d] for d in distance]).flatten()\n",
    "    else:\n",
    "        result = np.array([distance_for_each_point(array1[i], array2[i]) for i in range(array_length)])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# metrics between real results (in tests) and predicted\n",
    "def distance(test_results, predicted_results):\n",
    "    return distance_for_each_point(np.array(test_results), predicted_results).sum() / TEST_SAMPLE_NUMBER / 2\n",
    "\n",
    "# score between real results (in tests) and predicted\n",
    "def score(test_results, predicted_results, baseline_results):\n",
    "    test_results = np.array(test_results)\n",
    "    predicted_results = np.array(predicted_results)\n",
    "    baseline_results = np.array(baseline_results)\n",
    "    return (1 - ((test_results - predicted_results) ** 2).sum()/((test_results - baseline_results) ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data may be present as [n_features * n_samples] or [n_samples * n_features] \n",
    "# usually algorithms require second variant but I prefer first\n",
    "\n",
    "def to_model(df):\n",
    "    df = np.array(df).T\n",
    "    return df.reshape((int(df.shape[0] / 2), int(df.shape[1] * 2)))\n",
    "\n",
    "def to_model_with_features(df):\n",
    "    df = np.array(df)\n",
    "    movement_vector = np.diff(df,axis=0)\n",
    "    speed = np.power(movement_vector, 2)\n",
    "    speed[:,::2] = -speed[:,::2]\n",
    "    speed = np.sqrt(np.diff(speed)[:,::2])\n",
    "    double_speed = np.zeros_like(movement_vector, dtype=np.float64)\n",
    "    double_speed[:,::2] = double_speed[:,1::2] = speed\n",
    "    last_speed = speed[-1:]\n",
    "    mean_speed = np.mean(speed, axis=0)\n",
    "    cort_coord = movement_vector / double_speed\n",
    "    angle = np.arctan2(cort_coord[:,::2], cort_coord[:,1::2]) / np.pi * 4\n",
    "    \n",
    "    return (np.concatenate((to_model(df), to_model(movement_vector), speed.T, last_speed.T, np.array([mean_speed]).T,\n",
    "                            angle.T), axis=1)).astype(int)\n",
    "def from_model(npa):\n",
    "    return npa.reshape((int(npa.shape[0] * 2), int(npa.shape[1] / 2))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_delta(data, results):\n",
    "    data = np.array(data)\n",
    "    results = np.array(results)\n",
    "    \n",
    "    delta_results = np.copy(results)\n",
    "    delta_results[0,:] -= data[-1,:]\n",
    "    delta_results[1:,:] -= results[:-1,:]\n",
    "    \n",
    "    return delta_results\n",
    "\n",
    "def unmake_delta(data, delta):\n",
    "    result = np.copy(np.array(delta))\n",
    "    \n",
    "    for i, row in enumerate(result):\n",
    "        if i == 0:\n",
    "            result[i] += np.array(data)[-1,:]\n",
    "        else:\n",
    "            result[i] += result[i-1]\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first simply algorithm to get some start\n",
    "# more about it you can find in 'baseline_distance_between_real_points_and_predicted.ipynb'\n",
    "def baseline(test_data, start_point_index=0, number_of_points_to_return=5):\n",
    "    error_template = \"Start point index should be less than last point. Start point index: {st}, last point index: {end}\"\n",
    "    assert (start_point_index < len(test_data) - 1), error_template.format(st=start_point_index, end=len(test_data) - 1)\n",
    "    \n",
    "    start_point = np.array(test_data)[start_point_index]\n",
    "    last_but_one_point = np.array(test_data)[-2]\n",
    "    end_point = np.array(test_data)[-1]\n",
    "    \n",
    "    distance = distance_for_each_point(end_point, start_point)\n",
    "    normalized_motion_vector = (end_point - start_point) / distance\n",
    "    normalized_motion_vector[np.where(distance == 0)] = 0\n",
    "    last_vector_length = distance_for_each_point(end_point, last_but_one_point)\n",
    "    \n",
    "    motion_vector = normalized_motion_vector * last_vector_length\n",
    "    result = []\n",
    "    for i in range(number_of_points_to_return):\n",
    "        result.append(end_point + (i + 1) * motion_vector)\n",
    "        \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_results(function, test_results, parameter_name, list_of_values, **other_parameters):\n",
    "    result = [] # we will keep results for each configuration here\n",
    "\n",
    "    for i, value in enumerate(list_of_values):\n",
    "        print(\"{cur}/{num}: {t}\".format(cur=i+1, num=len(list_of_values), t=strftime(\"%Y-%m-%d %H:%M:%S\", localtime())))\n",
    "        other_parameters[parameter_name] = value        \n",
    "        predicted_results = function(**other_parameters)\n",
    "        np.save(\"src/Logs/{date}_predicted_coordinates_{function_name}_{parameter_name}_\".format(\n",
    "                date=strftime(\"%Y%m%d\", localtime()), function_name=function.__name__, parameter_name=parameter_name)\n",
    "                + str(value), predicted_results)\n",
    "        result.append(score(test_results, predicted_results, baseline(other_parameters['test_data'], start_point_index=2)))\n",
    "        \n",
    "    print(\"done! {time}\".format(time=strftime(\"%Y-%m-%d %H:%M:%S\", localtime())))\n",
    "    print(\"Results: {}\".format(result))\n",
    "        \n",
    "    ind = list_of_values\n",
    "    number = len(ind)\n",
    "    width = 2 / number\n",
    "    result_bar = plt.bar(range(number), result, width, color='g')\n",
    "\n",
    "    plt.ylabel('Average difference')\n",
    "    plt.xlabel(parameter_name)\n",
    "    plt.title(\"Difference between real points and predicted by {parameter_name} in {function_name}\".format(\n",
    "        function_name=function.__name__, parameter_name=parameter_name))\n",
    "    plt.xticks(np.array(range(number)) + width/2, ind)\n",
    "    plt.savefig(\"src/Plots/{date}_{function_name}_score_by_{parameter_name}_with_features.png\".format(\n",
    "        date=strftime(\"%Y%m%d\", localtime()), function_name=function.__name__, parameter_name=parameter_name))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading data from .csv files\n",
    "\n",
    "test_data = pd.read_csv('src/test_data_points.csv', index_col=0)\n",
    "train_data = pd.read_csv('src/train_data_points.csv', index_col=0)\n",
    "\n",
    "# split ten frames in input and output data (we want to predict output by input)\n",
    "\n",
    "test_results = test_data[5:10]\n",
    "test_data = test_data[:5]\n",
    "\n",
    "train_results = train_data[5:10]\n",
    "train_data = train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for transfer function you could use some of these: https://github.com/torch/nn/blob/master/doc/transfer.md\n",
    "\n",
    "def mult_layers_nn(train_data, train_results, test_data, hidden_nodes_num, learning_rate, epoch_num):\n",
    "    train_data_model = to_model_with_features(train_data)\n",
    "    train_results_model = to_model(make_delta(train_data, train_results))\n",
    "    test_data_model = to_model_with_features(test_data)\n",
    "    \n",
    "    ninputs = to_model_with_features(test_data).shape[1]\n",
    "    noutputs = 10\n",
    "    \n",
    "    # define model (predictor): \n",
    "    transfer_function=nn.Tanh()    \n",
    "    mlp = nn.Sequential()\n",
    "    mlp._add(nn.Linear(ninputs, hidden_nodes_num)) \n",
    "    mlp._add(transfer_function)\n",
    "    mlp._add(nn.Linear(hidden_nodes_num, hidden_nodes_num))\n",
    "    mlp._add(transfer_function)\n",
    "    mlp._add(nn.Linear(hidden_nodes_num, hidden_nodes_num))\n",
    "    mlp._add(transfer_function)\n",
    "    mlp._add(nn.Linear(hidden_nodes_num, hidden_nodes_num))\n",
    "    mlp._add(transfer_function)\n",
    "    mlp._add(nn.Linear(hidden_nodes_num, noutputs))\n",
    "    mlp._cuda()\n",
    "\n",
    "    # define a loss function to be minimized (mean-square error between predictions and groundtruth labels)\n",
    "    criterion = nn.MSECriterion() \n",
    "    criterion._cuda()\n",
    "    \n",
    "    crit_change = []\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        # get sample\n",
    "        inputs = (torch.fromNumpyArray(train_data_model.astype(float)))._cuda()\n",
    "        outputs = (torch.fromNumpyArray(train_results_model.astype(float)))._cuda()\n",
    "\n",
    "        # feed it to the neural network and the criterion\n",
    "        crit_change.append(criterion._forward(mlp._forward(inputs), outputs))\n",
    "\n",
    "        # train over this example in 3 steps\n",
    "        # (1) zero the accumulation of the gradients\n",
    "        mlp._zeroGradParameters()\n",
    "        # (2) accumulate gradients\n",
    "        mlp._backward(inputs, criterion.backward(criterion, mlp.output, outputs))\n",
    "\n",
    "        # (3) update parameters with a learning rate\n",
    "        mlp._updateParameters(learning_rate)\n",
    "        \n",
    "    result = (mlp._forward((torch.fromNumpyArray(test_data_model.astype(float)))._cuda())).asNumpyArray()\n",
    "    \n",
    "    crit_change = np.array(crit_change)\n",
    "    print(crit_change[::int(epoch_num / 10)])\n",
    "        \n",
    "    return unmake_delta(test_data, from_model(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 549.76611328  547.04003906  546.90020752  546.76025391  546.62310791\n",
      "  546.41265869  546.2020874   546.00024414  545.81085205  545.58612061]\n"
     ]
    }
   ],
   "source": [
    "result = mult_layers_nn(train_data=train_data, train_results=train_results, test_data=test_data, hidden_nodes_num=100,\n",
    "                     learning_rate=0.0001, epoch_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 549.66156006  544.44030762  543.11364746  542.11651611  542.77307129\n",
      "  542.91906738  542.71173096  543.13085938  543.33959961  538.88330078]\n"
     ]
    }
   ],
   "source": [
    "result = mult_layers_nn(train_data=train_data, train_results=train_results, test_data=test_data, hidden_nodes_num=100,\n",
    "                     learning_rate=0.001, epoch_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 549.70361328  368.43661499  367.50036621  366.86523438  366.42251587\n",
      "  366.10870361  365.88290405  365.71755981  365.59423828  365.50033569]\n"
     ]
    }
   ],
   "source": [
    "result = mult_layers_nn(train_data=train_data, train_results=train_results, test_data=test_data, hidden_nodes_num=100,\n",
    "                     learning_rate=0.1, epoch_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.49696899e+02   6.33108378e+13   3.56993981e+29              nan\n",
      "              nan              nan              nan              nan\n",
      "              nan              nan]\n"
     ]
    }
   ],
   "source": [
    "result = mult_layers_nn(train_data=train_data, train_results=train_results, test_data=test_data, hidden_nodes_num=100,\n",
    "                     learning_rate=1, epoch_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  549.47949219  1722.27734375  1712.2421875   1706.44970703  1561.6776123\n",
      "   946.43664551   925.80541992   918.46594238   939.63635254   958.83605957]\n"
     ]
    }
   ],
   "source": [
    "result = mult_layers_nn(train_data=train_data, train_results=train_results, test_data=test_data, hidden_nodes_num=100,\n",
    "                     learning_rate=0.1, epoch_num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 549.67004395  336.18185425  354.39868164  360.91418457  349.86746216\n",
      "  351.91119385  353.91931152  343.92974854  325.59405518  335.49816895]\n"
     ]
    }
   ],
   "source": [
    "result = mult_layers_nn(train_data=train_data, train_results=train_results, test_data=test_data, hidden_nodes_num=100,\n",
    "                     learning_rate=0.01, epoch_num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_results(function=mult_layers_nn, test_results=train_results, parameter_name=\"epoch_num\",\n",
    "                list_of_values=[10, 25, 50, 100], train_data=train_data, train_results=train_results,\n",
    "                test_data=train_data, learning_rate=0.0001, hidden_nodes_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_results(function=mult_layers_nn, test_results=test_results, parameter_name=\"hidden_nodes_num\",\n",
    "                list_of_values=[50, 100], train_data=train_data, train_results=train_results,\n",
    "                test_data=test_data, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
